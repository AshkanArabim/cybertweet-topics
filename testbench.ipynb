{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad5f31-5235-4528-819e-ce6a85d6e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave gensim for later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42db7b16-feee-4517-8a1d-fb685b38d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08adcc67-c4b8-40ac-a690-73ec5f430bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# documentation:\n",
    "# https://radimrehurek.com/gensim/downloader.html\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "# model = api.load(\"glove-twitter-25\")\n",
    "model = api.load(\"glove-wiki-gigaword-100\") # much, MUCH cleaner data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a459f6c-c71f-4051-9bdb-78597dd0f803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpora': {'20-newsgroups': {'checksum': 'c92fd4f6640a86d5ba89eaad818a9891',\n",
      "                               'description': 'The notorious collection of '\n",
      "                                              'approximately 20,000 newsgroup '\n",
      "                                              'posts, partitioned (nearly) '\n",
      "                                              'evenly across 20 different '\n",
      "                                              'newsgroups.',\n",
      "                               'fields': {'data': '',\n",
      "                                          'id': 'original id inferred from '\n",
      "                                                'folder name',\n",
      "                                          'set': 'marker of original split '\n",
      "                                                 \"(possible values 'train' and \"\n",
      "                                                 \"'test')\",\n",
      "                                          'topic': 'name of topic (20 variant '\n",
      "                                                   'of possible values)'},\n",
      "                               'file_name': '20-newsgroups.gz',\n",
      "                               'file_size': 14483581,\n",
      "                               'license': 'not found',\n",
      "                               'num_records': 18846,\n",
      "                               'parts': 1,\n",
      "                               'read_more': ['http://qwone.com/~jason/20Newsgroups/'],\n",
      "                               'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py',\n",
      "                               'record_format': 'dict'},\n",
      "             '__testing_matrix-synopsis': {'checksum': '1767ac93a089b43899d54944b07d9dc5',\n",
      "                                           'description': '[THIS IS ONLY FOR '\n",
      "                                                          'TESTING] Synopsis '\n",
      "                                                          'of the movie '\n",
      "                                                          'matrix.',\n",
      "                                           'file_name': '__testing_matrix-synopsis.gz',\n",
      "                                           'parts': 1,\n",
      "                                           'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
      "             '__testing_multipart-matrix-synopsis': {'checksum-0': 'c8b0c7d8cf562b1b632c262a173ac338',\n",
      "                                                     'checksum-1': '5ff7fc6818e9a5d9bc1cf12c35ed8b96',\n",
      "                                                     'checksum-2': '966db9d274d125beaac7987202076cba',\n",
      "                                                     'description': '[THIS IS '\n",
      "                                                                    'ONLY FOR '\n",
      "                                                                    'TESTING] '\n",
      "                                                                    'Synopsis '\n",
      "                                                                    'of the '\n",
      "                                                                    'movie '\n",
      "                                                                    'matrix.',\n",
      "                                                     'file_name': '__testing_multipart-matrix-synopsis.gz',\n",
      "                                                     'parts': 3,\n",
      "                                                     'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
      "             'fake-news': {'checksum': '5e64e942df13219465927f92dcefd5fe',\n",
      "                           'description': 'News dataset, contains text and '\n",
      "                                          'metadata from 244 websites and '\n",
      "                                          'represents 12,999 posts in total '\n",
      "                                          'from a specific window of 30 days. '\n",
      "                                          'The data was pulled using the '\n",
      "                                          \"webhose.io API, and because it's \"\n",
      "                                          'coming from their crawler, not all '\n",
      "                                          'websites identified by their BS '\n",
      "                                          'Detector are present in this '\n",
      "                                          'dataset. Data sources that were '\n",
      "                                          'missing a label were simply '\n",
      "                                          \"assigned a label of 'bs'. There are \"\n",
      "                                          '(ostensibly) no genuine, reliable, '\n",
      "                                          'or trustworthy news sources '\n",
      "                                          'represented in this dataset (so '\n",
      "                                          \"far), so don't trust anything you \"\n",
      "                                          'read.',\n",
      "                           'fields': {'author': 'author of story',\n",
      "                                      'comments': 'number of Facebook comments',\n",
      "                                      'country': 'data from webhose.io',\n",
      "                                      'crawled': 'date the story was archived',\n",
      "                                      'domain_rank': 'data from webhose.io',\n",
      "                                      'language': 'data from webhose.io',\n",
      "                                      'likes': 'number of Facebook likes',\n",
      "                                      'main_img_url': 'image from story',\n",
      "                                      'ord_in_thread': '',\n",
      "                                      'participants_count': 'number of '\n",
      "                                                            'participants',\n",
      "                                      'published': 'date published',\n",
      "                                      'replies_count': 'number of replies',\n",
      "                                      'shares': 'number of Facebook shares',\n",
      "                                      'site_url': 'site URL from BS detector',\n",
      "                                      'spam_score': 'data from webhose.io',\n",
      "                                      'text': 'text of story',\n",
      "                                      'thread_title': '',\n",
      "                                      'title': 'title of story',\n",
      "                                      'type': 'type of website (label from BS '\n",
      "                                              'detector)',\n",
      "                                      'uuid': 'unique identifier'},\n",
      "                           'file_name': 'fake-news.gz',\n",
      "                           'file_size': 20102776,\n",
      "                           'license': 'https://creativecommons.org/publicdomain/zero/1.0/',\n",
      "                           'num_records': 12999,\n",
      "                           'parts': 1,\n",
      "                           'read_more': ['https://www.kaggle.com/mrisdal/fake-news'],\n",
      "                           'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py',\n",
      "                           'record_format': 'dict'},\n",
      "             'patent-2017': {'checksum-0': '818501f0b9af62d3b88294d86d509f8f',\n",
      "                             'checksum-1': '66c05635c1d3c7a19b4a335829d09ffa',\n",
      "                             'description': 'Patent Grant Full Text. Contains '\n",
      "                                            'the full text including tables, '\n",
      "                                            \"sequence data and 'in-line' \"\n",
      "                                            'mathematical expressions of each '\n",
      "                                            'patent grant issued in 2017.',\n",
      "                             'file_name': 'patent-2017.gz',\n",
      "                             'file_size': 3087262469,\n",
      "                             'license': 'not found',\n",
      "                             'num_records': 353197,\n",
      "                             'parts': 2,\n",
      "                             'read_more': ['http://patents.reedtech.com/pgrbft.php'],\n",
      "                             'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py',\n",
      "                             'record_format': 'dict'},\n",
      "             'quora-duplicate-questions': {'checksum': 'd7cfa7fbc6e2ec71ab74c495586c6365',\n",
      "                                           'description': 'Over 400,000 lines '\n",
      "                                                          'of potential '\n",
      "                                                          'question duplicate '\n",
      "                                                          'pairs. Each line '\n",
      "                                                          'contains IDs for '\n",
      "                                                          'each question in '\n",
      "                                                          'the pair, the full '\n",
      "                                                          'text for each '\n",
      "                                                          'question, and a '\n",
      "                                                          'binary value that '\n",
      "                                                          'indicates whether '\n",
      "                                                          'the line contains a '\n",
      "                                                          'duplicate pair or '\n",
      "                                                          'not.',\n",
      "                                           'fields': {'id': 'the id of a '\n",
      "                                                            'training set '\n",
      "                                                            'question pair',\n",
      "                                                      'is_duplicate': 'the '\n",
      "                                                                      'target '\n",
      "                                                                      'variable, '\n",
      "                                                                      'set to '\n",
      "                                                                      '1 if '\n",
      "                                                                      'question1 '\n",
      "                                                                      'and '\n",
      "                                                                      'question2 '\n",
      "                                                                      'have '\n",
      "                                                                      'essentially '\n",
      "                                                                      'the '\n",
      "                                                                      'same '\n",
      "                                                                      'meaning, '\n",
      "                                                                      'and 0 '\n",
      "                                                                      'otherwise',\n",
      "                                                      'qid1': 'unique ids of '\n",
      "                                                              'each question',\n",
      "                                                      'qid2': 'unique ids of '\n",
      "                                                              'each question',\n",
      "                                                      'question1': 'the full '\n",
      "                                                                   'text of '\n",
      "                                                                   'each '\n",
      "                                                                   'question',\n",
      "                                                      'question2': 'the full '\n",
      "                                                                   'text of '\n",
      "                                                                   'each '\n",
      "                                                                   'question'},\n",
      "                                           'file_name': 'quora-duplicate-questions.gz',\n",
      "                                           'file_size': 21684784,\n",
      "                                           'license': 'probably '\n",
      "                                                      'https://www.quora.com/about/tos',\n",
      "                                           'num_records': 404290,\n",
      "                                           'parts': 1,\n",
      "                                           'read_more': ['https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs'],\n",
      "                                           'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py',\n",
      "                                           'record_format': 'dict'},\n",
      "             'semeval-2016-2017-task3-subtaskA-unannotated': {'checksum': '2de0e2f2c4f91c66ae4fcf58d50ba816',\n",
      "                                                              'description': 'SemEval '\n",
      "                                                                             '2016 '\n",
      "                                                                             '/ '\n",
      "                                                                             '2017 '\n",
      "                                                                             'Task '\n",
      "                                                                             '3 '\n",
      "                                                                             'Subtask '\n",
      "                                                                             'A '\n",
      "                                                                             'unannotated '\n",
      "                                                                             'dataset '\n",
      "                                                                             'contains '\n",
      "                                                                             '189,941 '\n",
      "                                                                             'questions '\n",
      "                                                                             'and '\n",
      "                                                                             '1,894,456 '\n",
      "                                                                             'comments '\n",
      "                                                                             'in '\n",
      "                                                                             'English '\n",
      "                                                                             'collected '\n",
      "                                                                             'from '\n",
      "                                                                             'the '\n",
      "                                                                             'Community '\n",
      "                                                                             'Question '\n",
      "                                                                             'Answering '\n",
      "                                                                             '(CQA) '\n",
      "                                                                             'web '\n",
      "                                                                             'forum '\n",
      "                                                                             'of '\n",
      "                                                                             'Qatar '\n",
      "                                                                             'Living. '\n",
      "                                                                             'These '\n",
      "                                                                             'can '\n",
      "                                                                             'be '\n",
      "                                                                             'used '\n",
      "                                                                             'as '\n",
      "                                                                             'a '\n",
      "                                                                             'corpus '\n",
      "                                                                             'for '\n",
      "                                                                             'language '\n",
      "                                                                             'modelling.',\n",
      "                                                              'fields': {'RelComments': [{'RELC_DATE': 'date '\n",
      "                                                                                                       'of '\n",
      "                                                                                                       'posting',\n",
      "                                                                                          'RELC_ID': 'comment '\n",
      "                                                                                                     'identifier',\n",
      "                                                                                          'RELC_USERID': 'identifier '\n",
      "                                                                                                         'of '\n",
      "                                                                                                         'the '\n",
      "                                                                                                         'user '\n",
      "                                                                                                         'posting '\n",
      "                                                                                                         'the '\n",
      "                                                                                                         'comment',\n",
      "                                                                                          'RELC_USERNAME': 'name '\n",
      "                                                                                                           'of '\n",
      "                                                                                                           'the '\n",
      "                                                                                                           'user '\n",
      "                                                                                                           'posting '\n",
      "                                                                                                           'the '\n",
      "                                                                                                           'comment',\n",
      "                                                                                          'RelCText': 'text '\n",
      "                                                                                                      'of '\n",
      "                                                                                                      'answer'}],\n",
      "                                                                         'RelQuestion': {'RELQ_CATEGORY': 'question '\n",
      "                                                                                                          'category, '\n",
      "                                                                                                          'according '\n",
      "                                                                                                          'to '\n",
      "                                                                                                          'the '\n",
      "                                                                                                          'Qatar '\n",
      "                                                                                                          'Living '\n",
      "                                                                                                          'taxonomy',\n",
      "                                                                                         'RELQ_DATE': 'date '\n",
      "                                                                                                      'of '\n",
      "                                                                                                      'posting',\n",
      "                                                                                         'RELQ_ID': 'question '\n",
      "                                                                                                    'indentifier',\n",
      "                                                                                         'RELQ_USERID': 'identifier '\n",
      "                                                                                                        'of '\n",
      "                                                                                                        'the '\n",
      "                                                                                                        'user '\n",
      "                                                                                                        'asking '\n",
      "                                                                                                        'the '\n",
      "                                                                                                        'question',\n",
      "                                                                                         'RELQ_USERNAME': 'name '\n",
      "                                                                                                          'of '\n",
      "                                                                                                          'the '\n",
      "                                                                                                          'user '\n",
      "                                                                                                          'asking '\n",
      "                                                                                                          'the '\n",
      "                                                                                                          'question',\n",
      "                                                                                         'RelQBody': 'body '\n",
      "                                                                                                     'of '\n",
      "                                                                                                     'question',\n",
      "                                                                                         'RelQSubject': 'subject '\n",
      "                                                                                                        'of '\n",
      "                                                                                                        'question'},\n",
      "                                                                         'THREAD_SEQUENCE': ''},\n",
      "                                                              'file_name': 'semeval-2016-2017-task3-subtaskA-unannotated.gz',\n",
      "                                                              'file_size': 234373151,\n",
      "                                                              'license': 'These '\n",
      "                                                                         'datasets '\n",
      "                                                                         'are '\n",
      "                                                                         'free '\n",
      "                                                                         'for '\n",
      "                                                                         'general '\n",
      "                                                                         'research '\n",
      "                                                                         'use.',\n",
      "                                                              'num_records': 189941,\n",
      "                                                              'parts': 1,\n",
      "                                                              'read_more': ['http://alt.qcri.org/semeval2016/task3/',\n",
      "                                                                            'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf',\n",
      "                                                                            'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
      "                                                                            'https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english'],\n",
      "                                                              'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py',\n",
      "                                                              'record_format': 'dict'},\n",
      "             'semeval-2016-2017-task3-subtaskBC': {'checksum': '701ea67acd82e75f95e1d8e62fb0ad29',\n",
      "                                                   'description': 'SemEval '\n",
      "                                                                  '2016 / 2017 '\n",
      "                                                                  'Task 3 '\n",
      "                                                                  'Subtask B '\n",
      "                                                                  'and C '\n",
      "                                                                  'datasets '\n",
      "                                                                  'contain '\n",
      "                                                                  'train+development '\n",
      "                                                                  '(317 '\n",
      "                                                                  'original '\n",
      "                                                                  'questions, '\n",
      "                                                                  '3,169 '\n",
      "                                                                  'related '\n",
      "                                                                  'questions, '\n",
      "                                                                  'and 31,690 '\n",
      "                                                                  'comments), '\n",
      "                                                                  'and test '\n",
      "                                                                  'datasets in '\n",
      "                                                                  'English. '\n",
      "                                                                  'The '\n",
      "                                                                  'description '\n",
      "                                                                  'of the '\n",
      "                                                                  'tasks and '\n",
      "                                                                  'the '\n",
      "                                                                  'collected '\n",
      "                                                                  'data is '\n",
      "                                                                  'given in '\n",
      "                                                                  'sections 3 '\n",
      "                                                                  'and 4.1 of '\n",
      "                                                                  'the task '\n",
      "                                                                  'paper '\n",
      "                                                                  'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf '\n",
      "                                                                  'linked in '\n",
      "                                                                  'section '\n",
      "                                                                  '“Papers” of '\n",
      "                                                                  'https://github.com/RaRe-Technologies/gensim-data/issues/18.',\n",
      "                                                   'fields': {'2016-dev': ['...'],\n",
      "                                                              '2016-test': ['...'],\n",
      "                                                              '2016-train': ['...'],\n",
      "                                                              '2017-test': ['...']},\n",
      "                                                   'file_name': 'semeval-2016-2017-task3-subtaskBC.gz',\n",
      "                                                   'file_size': 6344358,\n",
      "                                                   'license': 'All files '\n",
      "                                                              'released for '\n",
      "                                                              'the task are '\n",
      "                                                              'free for '\n",
      "                                                              'general '\n",
      "                                                              'research use',\n",
      "                                                   'num_records': -1,\n",
      "                                                   'parts': 1,\n",
      "                                                   'read_more': ['http://alt.qcri.org/semeval2017/task3/',\n",
      "                                                                 'http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf',\n",
      "                                                                 'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
      "                                                                 'https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english'],\n",
      "                                                   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py',\n",
      "                                                   'record_format': 'dict'},\n",
      "             'text8': {'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
      "                       'description': 'First 100,000,000 bytes of plain text '\n",
      "                                      'from Wikipedia. Used for testing '\n",
      "                                      'purposes; see wiki-english-* for proper '\n",
      "                                      'full Wikipedia datasets.',\n",
      "                       'file_name': 'text8.gz',\n",
      "                       'file_size': 33182058,\n",
      "                       'license': 'not found',\n",
      "                       'num_records': 1701,\n",
      "                       'parts': 1,\n",
      "                       'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
      "                       'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
      "                       'record_format': 'list of str (tokens)'},\n",
      "             'wiki-english-20171001': {'checksum-0': 'a7d7d7fd41ea7e2d7fa32ec1bb640d71',\n",
      "                                       'checksum-1': 'b2683e3356ffbca3b6c2dca6e9801f9f',\n",
      "                                       'checksum-2': 'c5cde2a9ae77b3c4ebce804f6df542c2',\n",
      "                                       'checksum-3': '00b71144ed5e3aeeb885de84f7452b81',\n",
      "                                       'description': 'Extracted Wikipedia '\n",
      "                                                      'dump from October 2017. '\n",
      "                                                      'Produced by `python -m '\n",
      "                                                      'gensim.scripts.segment_wiki '\n",
      "                                                      '-f '\n",
      "                                                      'enwiki-20171001-pages-articles.xml.bz2 '\n",
      "                                                      '-o wiki-en.gz`',\n",
      "                                       'fields': {'section_texts': 'list of '\n",
      "                                                                   'body of '\n",
      "                                                                   'sections',\n",
      "                                                  'section_titles': 'list of '\n",
      "                                                                    'titles of '\n",
      "                                                                    'sections',\n",
      "                                                  'title': 'Title of wiki '\n",
      "                                                           'article'},\n",
      "                                       'file_name': 'wiki-english-20171001.gz',\n",
      "                                       'file_size': 6516051717,\n",
      "                                       'license': 'https://dumps.wikimedia.org/legal.html',\n",
      "                                       'num_records': 4924894,\n",
      "                                       'parts': 4,\n",
      "                                       'read_more': ['https://dumps.wikimedia.org/enwiki/20171001/'],\n",
      "                                       'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py',\n",
      "                                       'record_format': 'dict'}},\n",
      " 'models': {'__testing_word2vec-matrix-synopsis': {'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
      "                                                   'description': '[THIS IS '\n",
      "                                                                  'ONLY FOR '\n",
      "                                                                  'TESTING] '\n",
      "                                                                  'Word '\n",
      "                                                                  'vecrors of '\n",
      "                                                                  'the movie '\n",
      "                                                                  'matrix.',\n",
      "                                                   'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
      "                                                   'parameters': {'dimensions': 50},\n",
      "                                                   'parts': 1,\n",
      "                                                   'preprocessing': 'Converted '\n",
      "                                                                    'to w2v '\n",
      "                                                                    'using a '\n",
      "                                                                    'preprocessed '\n",
      "                                                                    'corpus. '\n",
      "                                                                    'Converted '\n",
      "                                                                    'to w2v '\n",
      "                                                                    'format '\n",
      "                                                                    'with '\n",
      "                                                                    '`python3.5 '\n",
      "                                                                    '-m '\n",
      "                                                                    'gensim.models.word2vec '\n",
      "                                                                    '-train '\n",
      "                                                                    '<input_filename> '\n",
      "                                                                    '-iter 50 '\n",
      "                                                                    '-output '\n",
      "                                                                    '<output_filename>`.',\n",
      "                                                   'read_more': []},\n",
      "            'conceptnet-numberbatch-17-06-300': {'base_dataset': 'ConceptNet, '\n",
      "                                                                 'word2vec, '\n",
      "                                                                 'GloVe, and '\n",
      "                                                                 'OpenSubtitles '\n",
      "                                                                 '2016',\n",
      "                                                 'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
      "                                                 'description': 'ConceptNet '\n",
      "                                                                'Numberbatch '\n",
      "                                                                'consists of '\n",
      "                                                                'state-of-the-art '\n",
      "                                                                'semantic '\n",
      "                                                                'vectors (also '\n",
      "                                                                'known as word '\n",
      "                                                                'embeddings) '\n",
      "                                                                'that can be '\n",
      "                                                                'used directly '\n",
      "                                                                'as a '\n",
      "                                                                'representation '\n",
      "                                                                'of word '\n",
      "                                                                'meanings or '\n",
      "                                                                'as a starting '\n",
      "                                                                'point for '\n",
      "                                                                'further '\n",
      "                                                                'machine '\n",
      "                                                                'learning. '\n",
      "                                                                'ConceptNet '\n",
      "                                                                'Numberbatch '\n",
      "                                                                'is part of '\n",
      "                                                                'the '\n",
      "                                                                'ConceptNet '\n",
      "                                                                'open data '\n",
      "                                                                'project. '\n",
      "                                                                'ConceptNet '\n",
      "                                                                'provides lots '\n",
      "                                                                'of ways to '\n",
      "                                                                'compute with '\n",
      "                                                                'word '\n",
      "                                                                'meanings, one '\n",
      "                                                                'of which is '\n",
      "                                                                'word '\n",
      "                                                                'embeddings. '\n",
      "                                                                'ConceptNet '\n",
      "                                                                'Numberbatch '\n",
      "                                                                'is a snapshot '\n",
      "                                                                'of just the '\n",
      "                                                                'word '\n",
      "                                                                'embeddings. '\n",
      "                                                                'It is built '\n",
      "                                                                'using an '\n",
      "                                                                'ensemble that '\n",
      "                                                                'combines data '\n",
      "                                                                'from '\n",
      "                                                                'ConceptNet, '\n",
      "                                                                'word2vec, '\n",
      "                                                                'GloVe, and '\n",
      "                                                                'OpenSubtitles '\n",
      "                                                                '2016, using a '\n",
      "                                                                'variation on '\n",
      "                                                                'retrofitting.',\n",
      "                                                 'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
      "                                                 'file_size': 1225497562,\n",
      "                                                 'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
      "                                                 'num_records': 1917247,\n",
      "                                                 'parameters': {'dimension': 300},\n",
      "                                                 'parts': 1,\n",
      "                                                 'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
      "                                                               'https://github.com/commonsense/conceptnet-numberbatch',\n",
      "                                                               'http://conceptnet.io/'],\n",
      "                                                 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py'},\n",
      "            'fasttext-wiki-news-subwords-300': {'base_dataset': 'Wikipedia '\n",
      "                                                                '2017, UMBC '\n",
      "                                                                'webbase '\n",
      "                                                                'corpus and '\n",
      "                                                                'statmt.org '\n",
      "                                                                'news dataset '\n",
      "                                                                '(16B tokens)',\n",
      "                                                'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
      "                                                'description': '1 million word '\n",
      "                                                               'vectors '\n",
      "                                                               'trained on '\n",
      "                                                               'Wikipedia '\n",
      "                                                               '2017, UMBC '\n",
      "                                                               'webbase corpus '\n",
      "                                                               'and statmt.org '\n",
      "                                                               'news dataset '\n",
      "                                                               '(16B tokens).',\n",
      "                                                'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
      "                                                'file_size': 1005007116,\n",
      "                                                'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
      "                                                'num_records': 999999,\n",
      "                                                'parameters': {'dimension': 300},\n",
      "                                                'parts': 1,\n",
      "                                                'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
      "                                                              'https://arxiv.org/abs/1712.09405',\n",
      "                                                              'https://arxiv.org/abs/1607.01759'],\n",
      "                                                'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py'},\n",
      "            'glove-twitter-100': {'base_dataset': 'Twitter (2B tweets, 27B '\n",
      "                                                  'tokens, 1.2M vocab, '\n",
      "                                                  'uncased)',\n",
      "                                  'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
      "                                  'description': 'Pre-trained vectors based '\n",
      "                                                 'on  2B tweets, 27B tokens, '\n",
      "                                                 '1.2M vocab, uncased '\n",
      "                                                 '(https://nlp.stanford.edu/projects/glove/)',\n",
      "                                  'file_name': 'glove-twitter-100.gz',\n",
      "                                  'file_size': 405932991,\n",
      "                                  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                  'num_records': 1193514,\n",
      "                                  'parameters': {'dimension': 100},\n",
      "                                  'parts': 1,\n",
      "                                  'preprocessing': 'Converted to w2v format '\n",
      "                                                   'with `python -m '\n",
      "                                                   'gensim.scripts.glove2word2vec '\n",
      "                                                   '-i <fname> -o '\n",
      "                                                   'glove-twitter-100.txt`.',\n",
      "                                  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py'},\n",
      "            'glove-twitter-200': {'base_dataset': 'Twitter (2B tweets, 27B '\n",
      "                                                  'tokens, 1.2M vocab, '\n",
      "                                                  'uncased)',\n",
      "                                  'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
      "                                  'description': 'Pre-trained vectors based on '\n",
      "                                                 '2B tweets, 27B tokens, 1.2M '\n",
      "                                                 'vocab, uncased '\n",
      "                                                 '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                  'file_name': 'glove-twitter-200.gz',\n",
      "                                  'file_size': 795373100,\n",
      "                                  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                  'num_records': 1193514,\n",
      "                                  'parameters': {'dimension': 200},\n",
      "                                  'parts': 1,\n",
      "                                  'preprocessing': 'Converted to w2v format '\n",
      "                                                   'with `python -m '\n",
      "                                                   'gensim.scripts.glove2word2vec '\n",
      "                                                   '-i <fname> -o '\n",
      "                                                   'glove-twitter-200.txt`.',\n",
      "                                  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py'},\n",
      "            'glove-twitter-25': {'base_dataset': 'Twitter (2B tweets, 27B '\n",
      "                                                 'tokens, 1.2M vocab, uncased)',\n",
      "                                 'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
      "                                 'description': 'Pre-trained vectors based on '\n",
      "                                                '2B tweets, 27B tokens, 1.2M '\n",
      "                                                'vocab, uncased '\n",
      "                                                '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                 'file_name': 'glove-twitter-25.gz',\n",
      "                                 'file_size': 109885004,\n",
      "                                 'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                 'num_records': 1193514,\n",
      "                                 'parameters': {'dimension': 25},\n",
      "                                 'parts': 1,\n",
      "                                 'preprocessing': 'Converted to w2v format '\n",
      "                                                  'with `python -m '\n",
      "                                                  'gensim.scripts.glove2word2vec '\n",
      "                                                  '-i <fname> -o '\n",
      "                                                  'glove-twitter-25.txt`.',\n",
      "                                 'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                               'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py'},\n",
      "            'glove-twitter-50': {'base_dataset': 'Twitter (2B tweets, 27B '\n",
      "                                                 'tokens, 1.2M vocab, uncased)',\n",
      "                                 'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
      "                                 'description': 'Pre-trained vectors based on '\n",
      "                                                '2B tweets, 27B tokens, 1.2M '\n",
      "                                                'vocab, uncased '\n",
      "                                                '(https://nlp.stanford.edu/projects/glove/)',\n",
      "                                 'file_name': 'glove-twitter-50.gz',\n",
      "                                 'file_size': 209216938,\n",
      "                                 'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                 'num_records': 1193514,\n",
      "                                 'parameters': {'dimension': 50},\n",
      "                                 'parts': 1,\n",
      "                                 'preprocessing': 'Converted to w2v format '\n",
      "                                                  'with `python -m '\n",
      "                                                  'gensim.scripts.glove2word2vec '\n",
      "                                                  '-i <fname> -o '\n",
      "                                                  'glove-twitter-50.txt`.',\n",
      "                                 'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                               'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py'},\n",
      "            'glove-wiki-gigaword-100': {'base_dataset': 'Wikipedia 2014 + '\n",
      "                                                        'Gigaword 5 (6B '\n",
      "                                                        'tokens, uncased)',\n",
      "                                        'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
      "                                        'description': 'Pre-trained vectors '\n",
      "                                                       'based on Wikipedia '\n",
      "                                                       '2014 + Gigaword 5.6B '\n",
      "                                                       'tokens, 400K vocab, '\n",
      "                                                       'uncased '\n",
      "                                                       '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                        'file_name': 'glove-wiki-gigaword-100.gz',\n",
      "                                        'file_size': 134300434,\n",
      "                                        'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                        'num_records': 400000,\n",
      "                                        'parameters': {'dimension': 100},\n",
      "                                        'parts': 1,\n",
      "                                        'preprocessing': 'Converted to w2v '\n",
      "                                                         'format with `python '\n",
      "                                                         '-m '\n",
      "                                                         'gensim.scripts.glove2word2vec '\n",
      "                                                         '-i <fname> -o '\n",
      "                                                         'glove-wiki-gigaword-100.txt`.',\n",
      "                                        'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                      'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                        'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py'},\n",
      "            'glove-wiki-gigaword-200': {'base_dataset': 'Wikipedia 2014 + '\n",
      "                                                        'Gigaword 5 (6B '\n",
      "                                                        'tokens, uncased)',\n",
      "                                        'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
      "                                        'description': 'Pre-trained vectors '\n",
      "                                                       'based on Wikipedia '\n",
      "                                                       '2014 + Gigaword, 5.6B '\n",
      "                                                       'tokens, 400K vocab, '\n",
      "                                                       'uncased '\n",
      "                                                       '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                        'file_name': 'glove-wiki-gigaword-200.gz',\n",
      "                                        'file_size': 264336934,\n",
      "                                        'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                        'num_records': 400000,\n",
      "                                        'parameters': {'dimension': 200},\n",
      "                                        'parts': 1,\n",
      "                                        'preprocessing': 'Converted to w2v '\n",
      "                                                         'format with `python '\n",
      "                                                         '-m '\n",
      "                                                         'gensim.scripts.glove2word2vec '\n",
      "                                                         '-i <fname> -o '\n",
      "                                                         'glove-wiki-gigaword-200.txt`.',\n",
      "                                        'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                      'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                        'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py'},\n",
      "            'glove-wiki-gigaword-300': {'base_dataset': 'Wikipedia 2014 + '\n",
      "                                                        'Gigaword 5 (6B '\n",
      "                                                        'tokens, uncased)',\n",
      "                                        'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
      "                                        'description': 'Pre-trained vectors '\n",
      "                                                       'based on Wikipedia '\n",
      "                                                       '2014 + Gigaword, 5.6B '\n",
      "                                                       'tokens, 400K vocab, '\n",
      "                                                       'uncased '\n",
      "                                                       '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                        'file_name': 'glove-wiki-gigaword-300.gz',\n",
      "                                        'file_size': 394362229,\n",
      "                                        'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                        'num_records': 400000,\n",
      "                                        'parameters': {'dimension': 300},\n",
      "                                        'parts': 1,\n",
      "                                        'preprocessing': 'Converted to w2v '\n",
      "                                                         'format with `python '\n",
      "                                                         '-m '\n",
      "                                                         'gensim.scripts.glove2word2vec '\n",
      "                                                         '-i <fname> -o '\n",
      "                                                         'glove-wiki-gigaword-300.txt`.',\n",
      "                                        'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                      'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                        'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py'},\n",
      "            'glove-wiki-gigaword-50': {'base_dataset': 'Wikipedia 2014 + '\n",
      "                                                       'Gigaword 5 (6B tokens, '\n",
      "                                                       'uncased)',\n",
      "                                       'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
      "                                       'description': 'Pre-trained vectors '\n",
      "                                                      'based on Wikipedia 2014 '\n",
      "                                                      '+ Gigaword, 5.6B '\n",
      "                                                      'tokens, 400K vocab, '\n",
      "                                                      'uncased '\n",
      "                                                      '(https://nlp.stanford.edu/projects/glove/).',\n",
      "                                       'file_name': 'glove-wiki-gigaword-50.gz',\n",
      "                                       'file_size': 69182535,\n",
      "                                       'license': 'http://opendatacommons.org/licenses/pddl/',\n",
      "                                       'num_records': 400000,\n",
      "                                       'parameters': {'dimension': 50},\n",
      "                                       'parts': 1,\n",
      "                                       'preprocessing': 'Converted to w2v '\n",
      "                                                        'format with `python '\n",
      "                                                        '-m '\n",
      "                                                        'gensim.scripts.glove2word2vec '\n",
      "                                                        '-i <fname> -o '\n",
      "                                                        'glove-wiki-gigaword-50.txt`.',\n",
      "                                       'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
      "                                                     'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
      "                                       'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py'},\n",
      "            'word2vec-google-news-300': {'base_dataset': 'Google News (about '\n",
      "                                                         '100 billion words)',\n",
      "                                         'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
      "                                         'description': 'Pre-trained vectors '\n",
      "                                                        'trained on a part of '\n",
      "                                                        'the Google News '\n",
      "                                                        'dataset (about 100 '\n",
      "                                                        'billion words). The '\n",
      "                                                        'model contains '\n",
      "                                                        '300-dimensional '\n",
      "                                                        'vectors for 3 million '\n",
      "                                                        'words and phrases. '\n",
      "                                                        'The phrases were '\n",
      "                                                        'obtained using a '\n",
      "                                                        'simple data-driven '\n",
      "                                                        'approach described in '\n",
      "                                                        \"'Distributed \"\n",
      "                                                        'Representations of '\n",
      "                                                        'Words and Phrases and '\n",
      "                                                        'their '\n",
      "                                                        \"Compositionality' \"\n",
      "                                                        '(https://code.google.com/archive/p/word2vec/).',\n",
      "                                         'file_name': 'word2vec-google-news-300.gz',\n",
      "                                         'file_size': 1743563840,\n",
      "                                         'license': 'not found',\n",
      "                                         'num_records': 3000000,\n",
      "                                         'parameters': {'dimension': 300},\n",
      "                                         'parts': 1,\n",
      "                                         'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
      "                                                       'https://arxiv.org/abs/1301.3781',\n",
      "                                                       'https://arxiv.org/abs/1310.4546',\n",
      "                                                       'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
      "                                         'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py'},\n",
      "            'word2vec-ruscorpora-300': {'base_dataset': 'Russian National '\n",
      "                                                        'Corpus (about 250M '\n",
      "                                                        'words)',\n",
      "                                        'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
      "                                        'description': 'Word2vec Continuous '\n",
      "                                                       'Skipgram vectors '\n",
      "                                                       'trained on full '\n",
      "                                                       'Russian National '\n",
      "                                                       'Corpus (about 250M '\n",
      "                                                       'words). The model '\n",
      "                                                       'contains 185K words.',\n",
      "                                        'file_name': 'word2vec-ruscorpora-300.gz',\n",
      "                                        'file_size': 208427381,\n",
      "                                        'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
      "                                        'num_records': 184973,\n",
      "                                        'parameters': {'dimension': 300,\n",
      "                                                       'window_size': 10},\n",
      "                                        'parts': 1,\n",
      "                                        'preprocessing': 'The corpus was '\n",
      "                                                         'lemmatized and '\n",
      "                                                         'tagged with '\n",
      "                                                         'Universal PoS',\n",
      "                                        'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
      "                                                      'http://rusvectores.org/en/',\n",
      "                                                      'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
      "                                        'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py'}}}\n"
     ]
    }
   ],
   "source": [
    "# print(model.most_similar('ashkan')) # works!\n",
    "\n",
    "# testword = 'ashkan'\n",
    "# if testword in model:\n",
    "#     print(model[testword])\n",
    "# print(model.key_to_index['iran'])\n",
    "# print(model.index_to_key[399999])\n",
    "# print(len(model)) # set unseen words as the rarest word??\n",
    "\n",
    "# model.\n",
    "\n",
    "pprint(api.info())\n",
    "# model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b663abd-951c-4264-b516-6e0694c7cad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
